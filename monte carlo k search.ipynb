{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f74a619-8026-4118-bd8f-d61a2a22a8fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Monte Carlo Iterations: 100%|████████████| 1000/1000 [13:35:27<00:00, 48.93s/it]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cupy as cp\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tqdm import tqdm  # For tracking progress\n",
    "\n",
    "# Parameters\n",
    "n_iterations = 1000  # Number of Monte Carlo iterations\n",
    "k_values = range(1, 41)  # Reduced range of k values to evaluate to save memory\n",
    "batch_size = 1000  # Batch size for distance calculation\n",
    "\n",
    "def load_and_clean_data(filepath):\n",
    "    \"\"\"\n",
    "    Load dataset and clean unnecessary columns and duplicates.\n",
    "    \"\"\"\n",
    "    # Load the dataset\n",
    "    df = pd.read_csv(filepath)\n",
    "\n",
    "    # Remove rows with duplicate compositions, keeping only the first occurrence\n",
    "    df = df.drop_duplicates(subset='composition', keep='first').reset_index(drop=True)\n",
    "\n",
    "    # Drop unnecessary columns\n",
    "    columns_to_drop = ['composition', 'sample_id', 'prop_x', 'prop_y', 'unit_x', 'unit_y', \n",
    "                       'Temperature', 'Thermal Conductivity', 'comp_obj', 'atomic_fractions', \n",
    "                       'group', 'class']\n",
    "    df = df.drop(columns=columns_to_drop, errors='ignore').reset_index(drop=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "def monte_carlo_iteration(iteration, df, k_values):\n",
    "    \"\"\"\n",
    "    Perform one Monte Carlo iteration, including splitting data, scaling,\n",
    "    calculating distances, and determining applicability domain.\n",
    "    \"\"\"\n",
    "    # Step 1: Split the data into 80% training and 20% testing data\n",
    "    train_data, test_data = train_test_split(df, test_size=0.2, random_state=np.random.randint(0, 10000))\n",
    "\n",
    "    # Step 1.1: Apply StandardScaler to the training and test data\n",
    "    scaler = StandardScaler()\n",
    "    train_data_scaled = scaler.fit_transform(train_data)\n",
    "    test_data_scaled = scaler.transform(test_data)\n",
    "\n",
    "    # Convert training and test data to CuPy arrays for GPU acceleration\n",
    "    train_data_gpu = cp.array(train_data_scaled, dtype=cp.float32)\n",
    "    test_data_gpu = cp.array(test_data_scaled, dtype=cp.float32)\n",
    "\n",
    "    # Initialize a dictionary to store results for the current iteration\n",
    "    iteration_results = {k: {'within_domain': 0, 'out_of_domain': 0} for k in k_values}\n",
    "    \n",
    "    # Step 2: Calculate the entire distance matrix between all training samples in batches to save memory\n",
    "    n_train = train_data_gpu.shape[0]\n",
    "    full_distances = cp.full((n_train, n_train), cp.nan, dtype=cp.float32)\n",
    "\n",
    "    for start_idx in range(0, n_train, batch_size):\n",
    "        end_idx = min(start_idx + batch_size, n_train)\n",
    "        full_distances[start_idx:end_idx] = cp.linalg.norm(train_data_gpu[start_idx:end_idx, cp.newaxis, :] - train_data_gpu[cp.newaxis, :, :], axis=2)\n",
    "\n",
    "    cp.get_default_memory_pool().free_all_blocks()  # Free GPU memory after calculation\n",
    "\n",
    "    # Step 3: Exclude self-distances (set them to NaN)\n",
    "    cp.fill_diagonal(full_distances, cp.nan)\n",
    "\n",
    "    # Loop through each k value to calculate applicability domain\n",
    "    for k in k_values:\n",
    "        avg_knn_distances = cp.nanmean(cp.sort(full_distances, axis=1)[:, :k], axis=1)\n",
    "\n",
    "        # Step 5: Calculate reference value (Ref Val) using Q1 and Q3 of avg_knn_distances\n",
    "        Q1 = cp.percentile(avg_knn_distances, 25)\n",
    "        Q3 = cp.percentile(avg_knn_distances, 75)\n",
    "        IQR = Q3 - Q1\n",
    "        reference_value = Q3 + 1.5 * IQR\n",
    "\n",
    "        # Step 6: Filter the initial distance matrix based on the reference value to define the neighborhood\n",
    "        filtered_distances = cp.where(full_distances <= reference_value, full_distances, cp.nan)\n",
    "        Ki_values = cp.sum(~cp.isnan(filtered_distances), axis=1)\n",
    "\n",
    "        # Step 7: Calculate the threshold ti for each training sample\n",
    "        thresholds = cp.full(filtered_distances.shape[0], cp.nan, dtype=cp.float32)\n",
    "        for i in range(filtered_distances.shape[0]):\n",
    "            if Ki_values[i] > 0:\n",
    "                thresholds[i] = cp.nanmean(filtered_distances[i, :])\n",
    "\n",
    "        min_threshold = cp.nanmin(thresholds[~cp.isnan(thresholds)])\n",
    "        thresholds = cp.where(cp.isnan(thresholds), min_threshold, thresholds)\n",
    "\n",
    "        # Step 8: Evaluate the applicability domain on the test data (20%)\n",
    "        test_distances = cp.linalg.norm(test_data_gpu[:, cp.newaxis, :] - train_data_gpu[cp.newaxis, :, :], axis=2)\n",
    "        within_domain_mask = cp.any(test_distances <= thresholds[cp.newaxis, :], axis=1)\n",
    "        within_domain_count = cp.sum(within_domain_mask)\n",
    "        out_of_domain_count = test_data_gpu.shape[0] - within_domain_count\n",
    "\n",
    "        # Update iteration results\n",
    "        iteration_results[k]['within_domain'] = int(within_domain_count)\n",
    "        iteration_results[k]['out_of_domain'] = int(out_of_domain_count)\n",
    "\n",
    "    # Free GPU memory after each iteration\n",
    "    del full_distances, test_data_gpu, train_data_gpu\n",
    "    cp.get_default_memory_pool().free_all_blocks()\n",
    "\n",
    "    return iteration_results\n",
    "\n",
    "def main():\n",
    "    # Load and clean the data\n",
    "    df = load_and_clean_data(\"../training_data.csv\")\n",
    "\n",
    "    # Run Monte Carlo iterations\n",
    "    all_iterations_results = []\n",
    "    for iteration in tqdm(range(n_iterations), desc=\"Monte Carlo Iterations\"):\n",
    "        result = monte_carlo_iteration(iteration, df, k_values)\n",
    "        all_iterations_results.append(result)\n",
    "\n",
    "    # Store iteration details for each iteration separately\n",
    "    iteration_details = {k: [] for k in k_values}\n",
    "    for iteration_idx, result in enumerate(all_iterations_results):\n",
    "        for k in k_values:\n",
    "            iteration_details[k].append({'iteration': iteration_idx, 'within_domain': result[k]['within_domain'], 'out_of_domain': result[k]['out_of_domain']})\n",
    "\n",
    "    # Save iteration details to a CSV file\n",
    "    iteration_details_list = []\n",
    "    for k, details in iteration_details.items():\n",
    "        for detail in details:\n",
    "            iteration_details_list.append({'k': k, 'iteration': detail['iteration'], 'within_domain': detail['within_domain'], 'out_of_domain': detail['out_of_domain']})\n",
    "    iteration_details_df = pd.DataFrame(iteration_details_list)\n",
    "    iteration_details_df.to_csv(\"knn_iteration_details.csv\", index=False)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c3ef19-5108-46b9-87cc-b17226128eb1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
