{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9219168d-4eea-48c6-a4f6-2b36e9c5e178",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thresholds calculated and saved to 'knn_thresholds.csv'\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c2938a6ca0c4ccc8745359d8b37a3cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "StrToComposition:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/matminer/utils/data.py:326: UserWarning: MagpieData(impute_nan=False):\n",
      "In a future release, impute_nan will be set to True by default.\n",
      "                    This means that features that are missing or are NaNs for elements\n",
      "                    from the data source will be replaced by the average of that value\n",
      "                    over the available elements.\n",
      "                    This avoids NaNs after featurization that are often replaced by\n",
      "                    dataset-dependent averages.\n",
      "  warnings.warn(f\"{self.__class__.__name__}(impute_nan=False):\\n\" + IMPUTE_NAN_WARNING)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2928f33115204ee7a316b6987faed92d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ElementProperty:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The test sample falls within the applicability domain of 0 training samples\n",
      "Closest composition for U2Ti in training data: U65.4Zr34.6\n",
      "Top 10 features with the smallest differences:\n",
      "                         Feature  Difference\n",
      "0   MagpieData minimum NdValence         0.0\n",
      "1  MagpieData minimum NpUnfilled         0.0\n",
      "2  MagpieData maximum NpUnfilled         0.0\n",
      "3    MagpieData range NpUnfilled         0.0\n",
      "4     MagpieData mean NpUnfilled         0.0\n",
      "5     MagpieData mode NpUnfilled         0.0\n",
      "6  MagpieData minimum NdUnfilled         0.0\n",
      "7    MagpieData range NdUnfilled         0.0\n",
      "8      MagpieData mean NsValence         0.0\n",
      "9   MagpieData avg_dev NsValence         0.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cupy as cp\n",
    "from joblib import load\n",
    "from tqdm import tqdm\n",
    "from matminer.featurizers.conversions import StrToComposition\n",
    "from matminer.featurizers.composition.composite import ElementProperty\n",
    "\n",
    "def load_and_preprocess_data(file_path):\n",
    "    # Load the dataset from CSV file\n",
    "    df = pd.read_csv(file_path)\n",
    "    # Drop the 'composition' column as it will not be used in distance calculations\n",
    "    df_no_composition = df.drop(['composition'], axis=1)\n",
    "    return df, df_no_composition\n",
    "\n",
    "def convert_to_gpu_array(df_no_composition):\n",
    "    # Convert the dataframe to a CuPy GPU array for faster calculations\n",
    "    return cp.array(df_no_composition.values, dtype=cp.float32)\n",
    "\n",
    "def compute_blockwise_distance_matrix(df_gpu, block_size):\n",
    "    # Get the number of samples\n",
    "    n_samples = df_gpu.shape[0]\n",
    "    # Initialize the full distance matrix with NaN values\n",
    "    full_distances = cp.full((n_samples, n_samples), cp.nan, dtype=cp.float32)\n",
    "    # Calculate distances in blocks to manage memory usage\n",
    "    for i in range(0, n_samples, block_size):\n",
    "        for j in range(0, n_samples, block_size):\n",
    "            # Extract blocks of data for pairwise distance calculation\n",
    "            block_1 = df_gpu[i:i + block_size]\n",
    "            block_2 = df_gpu[j:j + block_size]\n",
    "            # Compute the pairwise Euclidean distance between two blocks\n",
    "            distances_block = cp.linalg.norm(block_1[:, cp.newaxis, :] - block_2[cp.newaxis, :, :], axis=2)\n",
    "            # Fill the appropriate part of the full distance matrix\n",
    "            full_distances[i:i + block_size, j:j + block_size] = distances_block\n",
    "    return full_distances\n",
    "\n",
    "def calculate_knn_thresholds(full_distances, k):\n",
    "    # Set diagonal distances (self-distances) to NaN\n",
    "    cp.fill_diagonal(full_distances, cp.nan)\n",
    "    # Sort distances for each sample to find the k-nearest neighbors\n",
    "    sorted_distances = cp.sort(full_distances, axis=1)\n",
    "    k_nearest_distances = sorted_distances[:, :k]\n",
    "    # Calculate the average distance to the k-nearest neighbors for each sample\n",
    "    avg_knn_distances = cp.nanmean(k_nearest_distances, axis=1)\n",
    "\n",
    "    # Calculate the first (Q1) and third (Q3) quartiles and the interquartile range (IQR)\n",
    "    Q1, Q3 = cp.percentile(avg_knn_distances, [25, 75])\n",
    "    IQR = Q3 - Q1\n",
    "    # Define the reference value for identifying outliers\n",
    "    reference_value = Q3 + 1.5 * IQR\n",
    "\n",
    "    # Filter distances to only include those below the reference value (for density estimation)\n",
    "    filtered_distances = cp.where(full_distances <= reference_value, full_distances, cp.nan)\n",
    "    # Count the number of neighbors within the reference value for each sample\n",
    "    Ki_values = cp.sum(~cp.isnan(filtered_distances), axis=1)\n",
    "\n",
    "    # Initialize thresholds with NaN values\n",
    "    thresholds = cp.full(filtered_distances.shape[0], cp.nan, dtype=cp.float32)\n",
    "    # Calculate the threshold for each sample where there are valid neighbors\n",
    "    for i in range(filtered_distances.shape[0]):\n",
    "        if Ki_values[i] > 0:\n",
    "            thresholds[i] = cp.nanmean(filtered_distances[i, :])\n",
    "\n",
    "    # Replace NaN thresholds with the minimum valid threshold\n",
    "    min_threshold = cp.nanmin(thresholds[~cp.isnan(thresholds)])\n",
    "    thresholds = cp.where(cp.isnan(thresholds), min_threshold, thresholds)\n",
    "    return thresholds\n",
    "\n",
    "def save_thresholds(thresholds, file_path):\n",
    "    # Move the thresholds from GPU to CPU for saving\n",
    "    thresholds_cpu = thresholds.get()\n",
    "    # Save thresholds to a CSV file\n",
    "    thresholds_df = pd.DataFrame({'sample_index': np.arange(len(thresholds_cpu)), 'threshold': thresholds_cpu})\n",
    "    thresholds_df.to_csv(file_path, index=False)\n",
    "    print(f\"Thresholds calculated and saved to '{file_path}'\")\n",
    "\n",
    "def prepare_test_sample(material):\n",
    "    # Create a new DataFrame for the test material\n",
    "    df_new = pd.DataFrame({'material': [material]})\n",
    "    # Convert material strings to composition objects\n",
    "    df_new = StrToComposition().featurize_dataframe(df_new, \"material\", ignore_errors=True)\n",
    "    # Apply Magpie feature set after converting to composition\n",
    "    magpie_featurizer = ElementProperty.from_preset(\"magpie\")\n",
    "    df_new = magpie_featurizer.featurize_dataframe(df_new, \"composition\", ignore_errors=True)\n",
    "    # Drop unnecessary columns ('material' and 'composition')\n",
    "    df_new = df_new.drop(['material', 'composition'], axis=1)\n",
    "    return df_new\n",
    "\n",
    "def scale_test_sample(df_new, scaler_path, df_no_composition_columns):\n",
    "    # Load the scaler to apply the same scaling as the training data\n",
    "    scaler = load(scaler_path)\n",
    "    # Scale the new data\n",
    "    u2ti_scaled = scaler.transform(df_new)\n",
    "    # Match the columns to the original training data structure\n",
    "    df_u2ti = pd.DataFrame(u2ti_scaled, columns=df_new.columns)[df_no_composition_columns]\n",
    "    return df_u2ti\n",
    "\n",
    "def evaluate_applicability_domain(df_u2ti, df_gpu, thresholds):\n",
    "    # Convert the scaled test sample to a GPU array\n",
    "    u2ti_scaled_gpu = cp.array(df_u2ti, dtype=cp.float32)\n",
    "    # Calculate distances between the test sample and all training samples\n",
    "    test_distances = cp.linalg.norm(u2ti_scaled_gpu - df_gpu, axis=1)\n",
    "    # Determine how many training samples fall within the applicability domain of the test sample\n",
    "    within_domain_mask = test_distances <= thresholds\n",
    "    within_domain_count = cp.sum(within_domain_mask).get()\n",
    "    print(f\"The test sample falls within the applicability domain of {within_domain_count} training samples\")\n",
    "    return test_distances\n",
    "\n",
    "def find_closest_sample(test_distances, df, df_no_composition, df_u2ti):\n",
    "    # Find the index of the closest training sample to the test sample\n",
    "    closest_sample_idx = cp.argmin(test_distances).get()\n",
    "    # Retrieve the composition of the closest training sample\n",
    "    closest_composition = df.loc[closest_sample_idx, \"composition\"]\n",
    "    print(f\"Closest composition for U2Ti in training data: {closest_composition}\")\n",
    "    # Get feature values of the closest training sample\n",
    "    closest_sample_features = df_no_composition.iloc[closest_sample_idx].values\n",
    "    # Convert the test sample to CPU for comparison\n",
    "    u2ti_scaled_cpu = df_u2ti.values[0]\n",
    "    # Calculate the absolute differences between the test sample and the closest training sample\n",
    "    feature_differences = np.abs(u2ti_scaled_cpu - closest_sample_features)\n",
    "    return feature_differences, df_no_composition.columns\n",
    "\n",
    "def display_closest_features(feature_differences, feature_names, top_n=10):\n",
    "    # Create a DataFrame to map feature differences to their respective names\n",
    "    difference_df = pd.DataFrame({'Feature': feature_names, 'Difference': feature_differences})\n",
    "    # Sort the features by their differences and reset index\n",
    "    closest_features = difference_df.sort_values(by=\"Difference\").reset_index(drop=True)\n",
    "    # Display the top N features with the smallest differences for analysis\n",
    "    print(f\"Top {top_n} features with the smallest differences:\")\n",
    "    print(closest_features.head(top_n))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Load and preprocess data\n",
    "    df, df_no_composition = load_and_preprocess_data(\"training_data_post_pearson.csv\")\n",
    "    df_gpu = convert_to_gpu_array(df_no_composition)\n",
    "\n",
    "    # Compute distances and calculate thresholds\n",
    "    full_distances = compute_blockwise_distance_matrix(df_gpu, block_size=1000)\n",
    "    thresholds = calculate_knn_thresholds(full_distances, k=17)\n",
    "    save_thresholds(thresholds, \"knn_thresholds.csv\")\n",
    "\n",
    "    # Prepare and scale the test sample\n",
    "    df_new = prepare_test_sample('U2Ti')\n",
    "    df_u2ti = scale_test_sample(df_new, \"scaler.pkl\", df_no_composition.columns)\n",
    "\n",
    "    # Evaluate applicability domain\n",
    "    test_distances = evaluate_applicability_domain(df_u2ti, df_gpu, thresholds)\n",
    "\n",
    "    # Find closest training sample and compare features\n",
    "    feature_differences, feature_names = find_closest_sample(test_distances, df, df_no_composition, df_u2ti)\n",
    "    display_closest_features(feature_differences, feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08f92f5-7b3d-4c64-91f4-498d59e985df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "175f63bb-0415-4b00-96d7-bf669676c078",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "824d9fe5a84f4c2e875c799fc188be7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "StrToComposition:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/matminer/utils/data.py:326: UserWarning: MagpieData(impute_nan=False):\n",
      "In a future release, impute_nan will be set to True by default.\n",
      "                    This means that features that are missing or are NaNs for elements\n",
      "                    from the data source will be replaced by the average of that value\n",
      "                    over the available elements.\n",
      "                    This avoids NaNs after featurization that are often replaced by\n",
      "                    dataset-dependent averages.\n",
      "  warnings.warn(f\"{self.__class__.__name__}(impute_nan=False):\\n\" + IMPUTE_NAN_WARNING)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fad6825b4f0b49bfb1e35b62849f2ca2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ElementProperty:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_newer = pd.DataFrame({'material': ['U65.4Zr34.6']})\n",
    "# Convert composition strings to Composition objects\n",
    "df_newer = StrToComposition().featurize_dataframe(df_newer, \"material\", ignore_errors=True)\n",
    "\n",
    "# Apply Magpie feature set right after converting to composition\n",
    "magpie_featurizer = ElementProperty.from_preset(\"magpie\")\n",
    "df_newer = magpie_featurizer.featurize_dataframe(df_newer, \"composition\", ignore_errors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "691fa4da-f43a-4f56-8575-dd211f00ba91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1.0\n",
       "Name: MagpieData minimum NdValence, dtype: float64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_newer[\"MagpieData minimum NdValence\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4844601c-7d53-4e08-8ba7-69f02bae6ef4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1.0\n",
       "Name: MagpieData minimum NdValence, dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new[\"MagpieData minimum NdValence\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ca3f1f-aa89-4bfb-b579-e849736f18ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
